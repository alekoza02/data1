\section{Exercises with Jupyter \& Machine Learning}

\subsection{Module 1}
\paragraph{TODO 1-2}
How do the MSE\_train and MSE\_test compare between the linear and quadratic model?
Try to change the size of the datasets (m=50, 200, 500) and see how MSE\_train and MSE\_test change. 

\begin{lstlisting}
for MSE_train_linear, MSE_test_linear, MSE_train_quadratic, MSE_test_quadratic, sample in zip(MSE_linears_train, MSE_linears_test, MSE_poly_train, MSE_poly_test, samples):

    print(f"RESULTS OF {sample}:")

    print(f"\tDelta between training (linear - quadratic): {abs(MSE_train_linear - MSE_train_quadratic)}")
    print(f"\tDelta between testing (linear - quadratic): {abs(MSE_test_linear - MSE_test_quadratic)}")


    print(f"\t{MSE_train_linear = }")
    print(f"\t{MSE_train_quadratic = }")
    print(f"\t{MSE_test_linear = }")
    print(f"\t{MSE_test_quadratic = }")

    print(f"\tThe best model is the {'linear' if MSE_test_linear < MSE_test_quadratic else 'quadratic'}")

    print(f"\n")
\end{lstlisting}

First exploration of basic ML algorithms. 
The Mean Squared Error (MSE) measures how well the model is able to explain the data it was trained on.
A low MSE means that the model fits well the data.
We should note that an MSE of 0 in training is absolute overfitting, resulting almost certainly in a poor performance in test environment. 
Overall we are testing which model has the LOWEST MSE in test environment.

\begin{table}[!ht]
\centering
% \resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\hline
$N$ & $\text{MSE}_\text{train, linear}$ & $\text{MSE}_\text{test, linear}$ & $\text{MSE}_\text{train, quad}$ & $\text{MSE}_\text{test, quad}$ \\
\hline
50    & \cellcolor{myred} 2.24 & \cellcolor{myred} 3.05 & \cellcolor{mygreen} 1.29 & \cellcolor{mygreen} 0.87 \\
100   & \cellcolor{myred} 3.17 & \cellcolor{myred} 2.24 & \cellcolor{mygreen} 0.96 & \cellcolor{mygreen} 1.22 \\
200   & \cellcolor{myred} 2.36 & \cellcolor{myred} 3.36 & \cellcolor{mygreen} 0.88 & \cellcolor{mygreen} 1.19 \\
500   & \cellcolor{myred} 2.76 & \cellcolor{myred} 2.72 & \cellcolor{mygreen} 1.08 & \cellcolor{mygreen} 0.85 \\
1000  & \cellcolor{myred} 2.90 & \cellcolor{myred} 2.93 & \cellcolor{mygreen} 1.00 & \cellcolor{mygreen} 1.13 \\
2000  & \cellcolor{myred} 2.74 & \cellcolor{myred} 2.64 & \cellcolor{mygreen} 0.99 & \cellcolor{mygreen} 0.99 \\
5000  & \cellcolor{myred} 2.74 & \cellcolor{myred} 2.73 & \cellcolor{mygreen} 0.97 & \cellcolor{mygreen} 0.97 \\
\hline
\end{tabular}%
% }
\caption{Comparison of Linear and Quadratic Models for Different Sample Sizes $N$. For all the sample sizes, the best model is quadratic.}
\end{table}

\paragraph{TODO 3}  
Compute the MSE for tree\_reg3 and tree\_reg4 models, both on the training set and on the test set. 

\begin{lstlisting}
results_trees3 = []

for result, tree_reg3 in zip(results_linear, tree_regs3):
    
    X_train, X_test, y_train, y_test = result

    y_train_pred = tree_reg3.predict(X_train)
    MSE_train_treereg3 = mean_squared_error(y_train, y_train_pred)
    y_test_pred = tree_reg3.predict(X_test)
    MSE_test_treereg3 = mean_squared_error(y_test, y_test_pred)
    results_trees3.append([MSE_train_treereg3, MSE_test_treereg3])

results_trees4 = []

for result, tree_reg4 in zip(results_linear, tree_regs4):
    
    X_train, X_test, y_train, y_test = result

    y_train_pred = tree_reg4.predict(X_train)
    MSE_train_treereg4 = mean_squared_error(y_train, y_train_pred)
    y_test_pred = tree_reg4.predict(X_test)
    MSE_test_treereg4 = mean_squared_error(y_test, y_test_pred)
    results_trees4.append([MSE_train_treereg4, MSE_test_treereg4])

for tree3, tree4, sample in zip(results_trees3, results_trees4, samples):

    MSE_train_tree3, MSE_test_tree3 = tree3
    MSE_train_tree4, MSE_test_tree4 = tree4

    print(f"RESULTS OF {sample}:")
    
    print(f"\t{MSE_train_tree3 = }")
    print(f"\t{MSE_test_tree3 = }")
    print(f"\t{MSE_train_tree4 = }")
    print(f"\t{MSE_test_tree4 = }")

    print(f"\n")
\end{lstlisting}

Demonstration of the overfitting problem using regularization in decision trees (\texttt{min\_samples\_leaf=10}). 
Since we generalized the code to support many different sample sizes, we tested it out with the following N: (50, 100, 200, 500, 1000, 2000, 5000).

\begin{table}[!ht]
\centering
% \resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c}
\hline
$N$ & $\text{MSE}_\text{train, N}$ & $\text{MSE}_\text{test, N}$ & $\text{MSE}_\text{train, Y}$ & $\text{MSE}_\text{test, Y}$ \\
\hline
50    & \cellcolor{mygreen} 0.00 & \cellcolor{mygreen} 2.61 & \cellcolor{myred} 2.54 & \cellcolor{myred} 3.07 \\
100   & \cellcolor{mygreen} 0.00 & \cellcolor{myred} 1.69 & \cellcolor{myred} 0.87 & \cellcolor{mygreen} 1.34 \\
200   & \cellcolor{mygreen} 0.00 & \cellcolor{myred} 2.35 & \cellcolor{myred} 0.73 & \cellcolor{mygreen} 1.56 \\
500   & \cellcolor{mygreen} 0.00 & \cellcolor{myred} 1.92 & \cellcolor{myred} 0.89 & \cellcolor{mygreen} 1.03 \\
1000  & \cellcolor{mygreen} 0.00 & \cellcolor{myred} 1.70 & \cellcolor{myred} 0.82 & \cellcolor{mygreen} 1.23 \\
2000  & \cellcolor{mygreen} 0.00 & \cellcolor{myred} 2.17 & \cellcolor{myred} 0.79 & \cellcolor{mygreen} 1.33 \\
5000  & \cellcolor{mygreen} 0.00 & \cellcolor{myred} 1.89 & \cellcolor{myred} 0.79 & \cellcolor{mygreen} 1.13 \\
\hline
\end{tabular}%
% }
\caption{Comparison of Tree Models (Y = with regularization, N = without regularization) for Different Sample Sizes $N$. Because of overfitting, the $\text{MSE}_{\text{train}}$ results are 0 (1째 column): the model fits exactly the data without understanding the general function. The 3째 column has a higher MSE, which is technically worse, but this is expected from a data with noise and a non-overfitting ML model. From the 2째 column we notice that it performs poorly compared to the regularized version (4째 column): the model is able to adapt to new problems with a higher success rate.}
\end{table}

\paragraph{TODO 4} 
Look at the online documentation of random forests in the scikitlearn library, find other hyperparameters.
Train 2 more random forests regularizing other hyperparameters. Plot the results and compute MSE on both training and test set. 
\\
We compared a Random Forest model with default hyperparameters vs a Random Forest model with specified hyperparameters. 

\begin{lstlisting}[style=pythonstyle]
rf_no_hyp, rf_with_hyp = [], []

for X_train, X_test, y_train, y_test in results_linear:
    rf_no_hyp_i = RandomForestRegressor(n_estimators=100, random_state=42)
    
    # using hyperparameters
    rf_with_hyp_i = RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        min_samples_split=10,
        min_samples_leaf=5,
        max_features=0.5,
        random_state=42
    )
    
    rf_no_hyp_i.fit(X_train, y_train)
    rf_with_hyp_i.fit(X_train, y_train)
    
    rf_no_hyp.append(rf_no_hyp_i)
    rf_with_hyp.append(rf_with_hyp_i)


for rf_no_hyp_i, rf_with_hyp_i, result in zip(rf_no_hyp, rf_with_hyp, results_linear):
    X_train, X_test, y_train, y_test = result
    plt.figure(figsize=(10, 4))
    
    plt.subplot(1, 2, 1)
    plot_regression_predictions(rf_no_hyp_i, X_train, y_train)
    plt.title("Random Forest (default)")
    
    plt.subplot(1, 2, 2)
    plot_regression_predictions(rf_with_hyp_i, X_train, y_train)
    plt.title("Random Forest (strong regularization)")
    
    plt.show()

results_rf_no_hyp = []
results_rf_with_hyp = []

for result, rf_no_hyp, rf_with_hyp in zip(results_linear, rf_no_hyp, rf_with_hyp):
    X_train, X_test, y_train, y_test = result
    
    # Model without hyperparameters
    y_train_pred = rf_no_hyp.predict(X_train)
    y_test_pred = rf_no_hyp.predict(X_test)
    mse_train_no_hyp = mean_squared_error(y_train, y_train_pred)
    mse_test_no_hyp = mean_squared_error(y_test, y_test_pred)
    results_rf_no_hyp.append([mse_train_no_hyp, mse_test_no_hyp])
    
    # Model with hyperparameters
    y_train_pred = rf_with_hyp.predict(X_train)
    y_test_pred = rf_with_hyp.predict(X_test)
    mse_train_with_hyp = mean_squared_error(y_train, y_train_pred)
    mse_test_with_hyp = mean_squared_error(y_test, y_test_pred)
    results_rf_with_hyp.append([mse_train_with_hyp, mse_test_with_hyp])

# Print results
for sample, ris_no_hyp, ris_with_hyp in zip(samples, results_rf_no_hyp, results_rf_with_hyp):
    mse_train_no_hyp, mse_test_no_hyp = ris_no_hyp
    mse_train_with_hyp, mse_test_with_hyp = ris_with_hyp
    print(f"RESULTS FOR {sample} instances:")
    print(f"   Default RF:      Train MSE = {mse_train_no_hyp:.4f},  Test MSE = {mse_test_no_hyp:.4f}")
    print(f"   Regularized RF:  Train MSE = {mse_train_with_hyp:.4f},  Test MSE = {mse_test_with_hyp:.4f}")
    print(f"   Overfitting reduced? {'YES' if mse_test_with_hyp < mse_test_no_hyp else 'NO'}\n")
\end{lstlisting}


\begin{table}[!ht]
\centering
\scriptsize
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\hline
$N$ & \text{Default RF Train MSE} & \text{Default RF Test MSE} & \text{Regularized RF Train MSE} & \text{Regularized RF Test MSE} & \text{Overfitting Reduced?} \\
\hline
50    & 0.2963 & \cellcolor{mygreen} 1.3113 & 1.3545 & \cellcolor{myred} 1.6156 & NO \\
100   & 0.1761 & \cellcolor{myred} 1.5028 & 0.7173 & \cellcolor{mygreen} 1.2067 & YES \\
200   & 0.1806 & \cellcolor{myred} 1.9127 & 0.6637 & \cellcolor{mygreen} 1.5084 & YES \\
500   & 0.1972 & \cellcolor{myred} 1.4768 & 0.8058 & \cellcolor{mygreen} 1.0025 & YES \\
1000  & 0.2117 & \cellcolor{myred} 1.4009 & 0.7574 & \cellcolor{mygreen} 1.1977 & YES \\
2000  & 0.1912 & \cellcolor{myred} 1.6286 & 0.7572 & \cellcolor{mygreen} 1.1690 & YES \\
5000  & 0.1979 & \cellcolor{myred} 1.4017 & 0.7985 & \cellcolor{mygreen} 1.0554 & YES \\
\hline
\end{tabular}%
}
\caption{Comparison of Default and Regularized Random Forest Models for Different Sample Sizes $N$.}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{images/rf_comparison_50.jpg}
    \caption{50 samples}
    \label{fig:rf_comparison_50}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{images/rf_comparison_100.jpg}
    \caption{100 samples}
    \label{fig:rf_comparison_100}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{images/rf_comparison_200.jpg}
    \caption{200 samples}
    \label{fig:rf_comparison_200}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{images/rf_comparison_500.jpg}
    \caption{500 samples}
    \label{fig:rf_comparison_500}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{images/rf_comparison_1000.jpg}
    \caption{1000 samples}
    \label{fig:rf_comparison_1000}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{images/rf_comparison_2000.jpg}
    \caption{2000 samples}
    \label{fig:rf_comparison_2000}
\end{figure}

\paragraph{TODO 5} 
Train a sequence of random forests with different values of the hyperparameter "n\_estimators" (20, 50, 100, 200, 300). 
In each case, compute the MSE on both the training set and the test set. Plot both MSEs vs n\_estimators.
How can I find the optimal value for the hyperparameter "n\_estimators"? 
For different values of \texttt{n\_estimators}, we observed that: 
\begin{itemize}
    \item Training MSE decreases (or stabilizes) as n\_estimators grows
    \item Test MSE usually decreases at first, then plateaus
    \item Optimal n\_estimators = point where test MSE stops improving significantly
\end{itemize}

\begin{lstlisting}[style=pythonstyle]
n_estimators_values = [20, 50, 100, 200, 300]
train_mses = []
test_mses = []

X_train, X_test, y_train, y_test = results_linear[-1]  # take the biggest split (results_linear has all datasets with increasing samples size)

for n in n_estimators_values:
    rf = RandomForestRegressor(n_estimators=n, random_state=42)
    rf.fit(X_train, y_train)
    
    train_pred = rf.predict(X_train)
    test_pred = rf.predict(X_test)
    
    train_mses.append(mean_squared_error(y_train, train_pred))
    test_mses.append(mean_squared_error(y_test, test_pred))

# Plot
plt.figure(figsize=(8, 5))
plt.plot(n_estimators_values, train_mses, 'o-', label='Training MSE', color='blue')
plt.plot(n_estimators_values, test_mses, 'o-', label='Test MSE', color='red')
plt.xlabel('n_estimators')
plt.ylabel('Mean Squared Error')
plt.title('Effect of n_estimators on Random Forest Performance')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{images/n_estimators_comparison.jpg}
    \label{fig:n_estimators_comparison}
    \caption{We see here how the amount of N estimators does not change particularly the outcome of the model. Probably the reason is that the problem was so simple that with 100 estimators, the Test MSE reached a stable plateau. We should probably focus on other hyperparameters to obtain a better result.}
\end{figure}

\paragraph{TODO 6 (optional)} 
As I have many hyperparameters in a Random Forest model, how can I find the best values for all of them? 
Repeat the above exercise with another hyperparameter. 

We chose the following hyperparameter: \texttt{max\_depth\_value}.

\begin{lstlisting}[style=pythonstyle]
max_depth_values = [3, 5, 8, 10, 15, 20, None]
train_mses_depth = []
test_mses_depth = []

for md in max_depth_values:
    rf = RandomForestRegressor(n_estimators=200, max_depth=md, random_state=42)
    rf.fit(X_train, y_train)
    
    train_mses_depth.append(mean_squared_error(y_train, rf.predict(X_train)))
    test_mses_depth.append(mean_squared_error(y_test, rf.predict(X_test)))

plt.figure(figsize=(8, 5))
plt.plot([str(d) for d in max_depth_values], train_mses_depth, 'o-', label='Training MSE')
plt.plot([str(d) for d in max_depth_values], test_mses_depth, 'o-', label='Test MSE')
plt.xlabel('max_depth')
plt.ylabel('MSE')
plt.title('Random Forest Performance vs max_depth')
plt.legend()
plt.grid(True, alpha=0.3)
plt.xticks(rotation=45)
plt.show()
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{images/max_depth_comparison.jpg}
    \label{fig:max_depth_comparison}
    \caption{\texttt{None} means that no maximum depth is enforced. As the maximum depth increases, the training MSE decreases because the trees overfit the data, because of this overfitting the MSE of the test set increases as the complexity of the model increases. Since this dataset is generated from a simple noisy parabola, a smaller depth provides the best bias-variance trade-off: the best max\_depth is 5, since we got the smallest MSE in the test dataset.}
\end{figure}

\subsection{Module 2}

\paragraph{TODO 1} 
Predict the class for another value of petal width. We test our own value to check the train dataset.

\begin{lstlisting}[style=pythonstyle]
    y_new_prediction = log_reg.predict([[6.0]]) # expected 1 -> Virginica
    print(f"The flower is {"" if y_new_prediction else "NOT "}Virginica")
\end{lstlisting}
output: \texttt{The flower is Virginica}

\paragraph{TODO 2} 
Calculate the probabilities for another value of petal width. 

\begin{lstlisting}[style=pythonstyle]
    y_proba_1 = log_reg.predict_proba([[6.0]])
    y_proba_1
\end{lstlisting}
output: \texttt{array([[6.83166501e-09, 9.99999993e-01]])}, meaning the model is 99.9999993\% sure the answer is the flower is Virginica.
\paragraph{TODO 3} 
Retrain a binary LogisticRegression classifier with only one feature as we just did, but first of all split the dataset into a training set (80\% of the data) and test set (20\% of the data).
Use the train\_test\_split function from scikit learn as in example 1.
After training, recompute the confusion matrix and scores on both the training set and the test set.
Can you comment on the differences? Obtain the confusion matrices of the model:

\begin{lstlisting}[style=pythonstyle]
# we choose the training on the petal length
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix



X = iris["data"][:, 2][:, None]  # petal length, take the column n 2
y = (iris["target"] == 2).astype(np.int64)  # this tests if X is = 2. 1 = True if Iris virginica, else 0 = False
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y) # we divide in test and train sets

log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train, y_train)

y_train_pred = log_reg.predict(X_train)
y_test_pred = log_reg.predict(X_test)

j = confusion_matrix(y_train, y_train_pred)
k = confusion_matrix(y_test, y_test_pred)
print(j, "\n",k)
\end{lstlisting}

\noindent output:

\[
j = \begin{pmatrix}
77 & 3 \\
2 & 38
\end{pmatrix}
\qquad
k = \begin{pmatrix}
19 & 1 \\
1 & 9
\end{pmatrix}
\]

\noindent comments:
\begin{itemize}
    \item The values on the anti-diagonal are low and similar between each other. This means a good accuracy  because we have a low number of false positives and false negatives, and are not really different (3, 2 and 1,1) which means that probably the model is not biased.
    \item After analyzing the dataset distribution, however, we see that Virginica is only 1/3 of the whole dataset; this means that the values of false negatives and false positives cannot be taken like that when the dataset is unbalanced
    \item As we expected, we obtained that the true positives are about 33\% of the set, while the remaining are the true negatives. 
    \item This is because 1/3 of the dataset is Virginica (true positive) and the remaining 2/3 are not Virginica (true negatives)
\end{itemize}

\paragraph{TODO 4}
Retrain a binary LogisticRegression classifier but with all the 4 features in the original iris dataset and splitting into a training set (80\% of the data) and test set (20\% of the data). 
Use the train\_test\_split function from scikit learn as in example 1.
After training, recompute the confusion matrix and scores on both the training set and the test set.
How do the scores compare with the previous classifier (with only one feature)? Repeat the previous experience but with all the attributes all-together.

\begin{lstlisting}[style=pythonstyle]
X = iris["data"] # we consider all the features
y = (iris["target"] == 2).astype(np.int64)  # this tests if X is = 2. 1 = True if Iris virginica, else 0 = False
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y) # we divide in test and train sets

log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train, y_train)

y_train_pred = log_reg.predict(X_train)
y_test_pred = log_reg.predict(X_test)

l = confusion_matrix(y_train, y_train_pred)
m = confusion_matrix(y_test, y_test_pred)
print(l, "\n",m)
\end{lstlisting}

\noindent output:
\[
l = \begin{pmatrix}
78 & 2 \\
1 & 39
\end{pmatrix}
\qquad
m = \begin{pmatrix}
20 & 0 \\
0 & 10
\end{pmatrix}
\]

\noindent comments:
\begin{itemize}
    \item We see a slight improvement of the false positives and false negatives, especially in the test set: we have zeros on the anti-diagonal.
\end{itemize}

\subsection{Module 3}

\paragraph{TODO 1}
Predict the class for another couple of values of petal length and width. 
\\
We test our own value to check the train dataset:

\begin{lstlisting}[style=pythonstyle]
y_new_1 = softmax_reg.predict([[4.0, 1]])    # petal length 1.0, petal width 0.5
y_new_1
\end{lstlisting}
output: \texttt{array([1])} $\to$ \texttt{versicolor}

\paragraph{TODO 2}
Calculate the probabilities for another couple of values of petal length and width.
\\ We decided to calculate related probability:

\begin{lstlisting}[style=pythonstyle]
y_proba_1 = softmax_reg.predict_proba([[4.0, 1]])
y_proba_1
\end{lstlisting}
output: \texttt{array([[0.02404399, 0.96249222, 0.01346379]])}, meaning the model is 96.249222\% sure the answer is the flower is Versicolor. Other probabilities are reffered to `setosa' and `virginica'.

\paragraph{TODO 3}
Now train a Random Forest multiclass classifier on the iris set, using all features available (sepal length and width, petal length and width). 
Use the RandomForestClassifier from scikit learn library. Set as n\_estimators=500 and all other hyperparameters with default values.
After training, calculate the confusion matrix and scores for this classifier. Compare the results with the softmax classifier. Try another value of n\_estimators.


\begin{lstlisting}
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

# Training of a Random Forest Multi Class Classifier using 2 different `n_estimators'
# The magic number 10 used in `rf_reg2' has been obtained by searching the minimum value necessary to obtain the same result as the n_estimator = 500 case
# ---

rf_reg1 = RandomForestClassifier(n_estimators= 500, random_state=42)
rf_reg2 = RandomForestClassifier(n_estimators= 10, random_state=42) 


# Training of a Soft Max Classifier
# ---
softmax_reg = LogisticRegression(random_state=42, max_iter=1000)

X = iris["data"]  # all the features
y = iris["target"]  

# Fitting all models with all the dataset
rf_reg1.fit(X, y)
rf_reg2.fit(X, y)
softmax_reg.fit(X, y)

# Predictions
y_pred_rf1 = rf_reg1.predict(X)
y_pred_rf2 = rf_reg2.predict(X)
y_pred_sftmax = softmax_reg.predict(X)

# Results: Confusion matrix and scores
conf_matrix_rf1 = confusion_matrix(y, y_pred_rf1)
prec_score_rf1 = precision_score(y, y_pred_rf1, average=None)
recall_score_rf1 = recall_score(y, y_pred_rf1, average=None)

conf_matrix_rf2 = confusion_matrix(y, y_pred_rf2)
prec_score_rf2 = precision_score(y, y_pred_rf2, average=None)
recall_score_rf2 = recall_score(y, y_pred_rf2, average=None)

conf_matrix_sftmax = confusion_matrix(y, y_pred_sftmax)
prec_score_sftmax = precision_score(y, y_pred_sftmax, average=None)
recall_score_sftmax = recall_score(y, y_pred_sftmax, average=None)

# Printing
print(f"{conf_matrix_rf1 = }")
print(f"{prec_score_rf1 = }")
print(f"{recall_score_rf1 = }")
print(f"{conf_matrix_rf2 = }")
print(f"{prec_score_rf2 = }")
print(f"{recall_score_rf2 = }")
print(f"{conf_matrix_sftmax = }")
print(f"{prec_score_sftmax = }")
print(f"{recall_score_sftmax = }")
\end{lstlisting}

\noindent output:
\begin{itemize}
\item
\begin{verbatim}
conf_matrix_rf1 = array([
    [50,  0,  0],
    [ 0, 50,  0],
    [ 0,  0, 50]
])
\end{verbatim}

\item
\begin{verbatim}
prec_score_rf1 = array([1., 1., 1.])
\end{verbatim}

\item
\begin{verbatim}
recall_score_rf1 = array([1., 1., 1.])
\end{verbatim}

\item
\begin{verbatim}
conf_matrix_rf2 = array([
    [50,  0,  0],
    [ 0, 50,  0],
    [ 0,  0, 50]
])
\end{verbatim}

\item
\begin{verbatim}
prec_score_rf2 = array([1., 1., 1.])
\end{verbatim}

\item
\begin{verbatim}
recall_score_rf2 = array([1., 1., 1.])
\end{verbatim}

\item
\begin{verbatim}
conf_matrix_sftmax = array([
    [50,  0,  0],
    [ 0, 47,  3],
    [ 0,  1, 49]
])
\end{verbatim}

\item
\begin{verbatim}
prec_score_sftmax = array([1., 0.97916667, 0.94230769])
\end{verbatim}

\item
\begin{verbatim}
recall_score_sftmax = array([1., 0.94, 0.98])
\end{verbatim}
\end{itemize}



\paragraph{TODO 4}
One great advantage of Random Forests is that they can tell you the relative importance of the features. 
Try to get it from the classifier you just trained. It is an attribute of the classifier called "feature\_importances\_".
If your classifier name is "randfor\_clf", then the feature importance values are in "randfor\_clf.feature\_importances\_".
Which is the most important feature for this classifier?

\noindent To analize what is going on in the model, we can access an attribute of the random forest called \texttt{feature\_importances\_} to see the values of how important each feature is for the model.

\begin{lstlisting}[style=pythonstyle]
rf_reg1.feature_importances_
rf_reg2.feature_importances_
\end{lstlisting}

Here are the results in percentage (importance). First value is rf\_reg1 and Second for rf\_reg2:

\begin{itemize}
    \item 11.249225 and 12.93\% $\to$ sepal length in cm    
    \item 2.311929 and 1.58\% $\to$ sepal width in cm    
    \item 44.103046 and 44.47\% $\to$ petal length in cm   $\Rightarrow$  most important feature for this classifer
    \item 42.3358 and 41.02\% $\to$ petal width in cm
\end{itemize}

\paragraph{TODO 5}

Reload the iris dataset and retrain a softmax and a random forest multiclass classifier, 
but this time first of all split the dataset into a training set and test set as in example 1. 
Then do the training of the training set only and calculate the scores and confusion matrix 
on both the training and test sets. Compare the results.


\begin{lstlisting}
from sklearn.model_selection import train_test_split

# For simmetry, the following exercise has been done using again the same `n_estimators' as the previous exercise.
# Training of a Random Forest Multi Class Classifier using 2 different `n_estimators'
# The magic number 10 used in `rf_reg2' has been obtained by searching the minimum value necessary to obtain the same result as the n_estimator = 500 case
# ---
rf_reg1 = RandomForestClassifier(n_estimators= 500, random_state=42)
rf_reg2 = RandomForestClassifier(n_estimators= 10, random_state=42)

X = iris["data"]  # use all the features
y = iris["target"]  

# we divide in test and train sets
# `stratify=y` preserves the class proportions in both splits,
# (here 1 `searched' sample every 3 samples on average)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)

# Fitting all models with training dataset
rf_reg1.fit(X_train, y_train)
rf_reg2.fit(X_train, y_train)

# Predictions
y_pred_rf1_train = rf_reg1.predict(X_train)
y_pred_rf1_test = rf_reg1.predict(X_test)
y_pred_rf2_train = rf_reg2.predict(X_train)
y_pred_rf2_test = rf_reg2.predict(X_test)

# Results: Confusion matrix and scores
train_conf_matrix_rf1 = confusion_matrix(y_train, y_pred_rf1_train)
train_prec_score_rf1 = precision_score(y_train, y_pred_rf1_train, average=None)
train_recall_score_rf1 = recall_score(y_train, y_pred_rf1_train, average=None)

train_conf_matrix_rf2 = confusion_matrix(y_train, y_pred_rf2_train)
train_prec_score_rf2 = precision_score(y_train, y_pred_rf2_train, average=None)
train_recall_score_rf2 = recall_score(y_train, y_pred_rf2_train, average=None)

test_conf_matrix_rf1 = confusion_matrix(y_test, y_pred_rf1_test)
test_prec_score_rf1 = precision_score(y_test, y_pred_rf1_test, average=None)
test_recall_score_rf1 = recall_score(y_test, y_pred_rf1_test, average=None)

test_conf_matrix_rf2 = confusion_matrix(y_test, y_pred_rf2_test)
test_prec_score_rf2 = precision_score(y_test, y_pred_rf2_test, average=None)
test_recall_score_rf2 = recall_score(y_test, y_pred_rf2_test, average=None)

# Printing
print(f"{train_conf_matrix_rf1 = }")
print(f"{train_prec_score_rf1 = }")
print(f"{train_recall_score_rf1 = }")
print(f"{train_conf_matrix_rf2 = }")
print(f"{train_prec_score_rf2 = }")
print(f"{train_recall_score_rf2 = }")
print(f"{test_conf_matrix_rf1 = }")
print(f"{test_prec_score_rf1 = }")
print(f"{test_recall_score_rf1 = }")
print(f"{test_conf_matrix_rf2 = }")
print(f"{test_prec_score_rf2 = }")
print(f"{test_recall_score_rf2 = }")
\end{lstlisting}

\noindent output:
\begin{itemize}
\item
\begin{verbatim}
train_conf_matrix_rf1 = array([
    [40,  0,  0],
    [ 0, 40,  0],
    [ 0,  0, 40]
])
\end{verbatim}

\item
\begin{verbatim}
train_prec_score_rf1 = array([1., 1., 1.])
\end{verbatim}

\item
\begin{verbatim}
train_recall_score_rf1 = array([1., 1., 1.])
\end{verbatim}

\item
\begin{verbatim}
train_conf_matrix_rf2 = array([
    [40,  0,  0],
    [ 0, 40,  0],
    [ 0,  0, 40]
])
\end{verbatim}

\item
\begin{verbatim}
train_prec_score_rf2 = array([1., 1., 1.])
\end{verbatim}

\item
\begin{verbatim}
train_recall_score_rf2 = array([1., 1., 1.])
\end{verbatim}

\item
\begin{verbatim}
test_conf_matrix_rf1 = array([
    [10,  0,  0],
    [ 0,  9,  1],
    [ 0,  1,  9]
])
\end{verbatim}

\item
\begin{verbatim}
test_prec_score_rf1 = array([1. , 0.9, 0.9])
\end{verbatim}

\item
\begin{verbatim}
test_recall_score_rf1 = array([1. , 0.9, 0.9])
\end{verbatim}

\item
\begin{verbatim}
test_conf_matrix_rf2 = array([
    [10,  0,  0],
    [ 0,  9,  1],
    [ 0,  0, 10]
])
\end{verbatim}

\item
\begin{verbatim}
test_prec_score_rf2 = array([1.    , 1.    , 0.90909091])
\end{verbatim}

\item
\begin{verbatim}
test_recall_score_rf2 = array([1. , 0.9, 1. ])
\end{verbatim}
\end{itemize}