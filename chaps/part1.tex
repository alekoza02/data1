\section{Exercises with Jupyter + ML}

\subsection{Module 1}
\paragraph{TODO 1-2} First exploration of basic ML algorithms. MSE which stands for Mean Squared Error, reppresents how well the predicts values. We should note that an MSE of 0 in training is absolute overfitting, resulting almost certinly in a poor performance in test environment. Overall we are testing which model has the LOWEST MSE in test environment.

\begin{table}[!ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c|c}
\hline
$N$ & $\text{MSE}_\text{train, linear}$ & $\text{MSE}_\text{train, quad}$ & $\text{MSE}_\text{test, linear}$ & $\text{MSE}_\text{test, quad}$ & \text{Best Model} \\
\hline
50    & \cellcolor{myred} 2.24 & \cellcolor{mygreen} 1.29 & \cellcolor{myred} 3.05 & \cellcolor{mygreen} 0.87 & \text{Quadratic} \\
100   & \cellcolor{myred} 3.17 & \cellcolor{mygreen} 0.96 & \cellcolor{myred} 2.24 & \cellcolor{mygreen} 1.22 & \text{Quadratic} \\
200   & \cellcolor{myred} 2.36 & \cellcolor{mygreen} 0.88 & \cellcolor{myred} 3.36 & \cellcolor{mygreen} 1.19 & \text{Quadratic} \\
500   & \cellcolor{myred} 2.76 & \cellcolor{mygreen} 1.08 & \cellcolor{myred} 2.72 & \cellcolor{mygreen} 0.85 & \text{Quadratic} \\
1000  & \cellcolor{myred} 2.90 & \cellcolor{mygreen} 1.00 & \cellcolor{myred} 2.93 & \cellcolor{mygreen} 1.13 & \text{Quadratic} \\
2000  & \cellcolor{myred} 2.74 & \cellcolor{mygreen} 0.99 & \cellcolor{myred} 2.64 & \cellcolor{mygreen} 0.99 & \text{Quadratic} \\
5000  & \cellcolor{myred} 2.74 & \cellcolor{mygreen} 0.97 & \cellcolor{myred} 2.73 & \cellcolor{mygreen} 0.97 & \text{Quadratic} \\
\hline
\end{tabular}%
}
\caption{Comparison of Linear and Quadratic Models for Different Sample Sizes $N$.}
\end{table}

\paragraph{TODO 3} Demonstration of the overfitting problem using regularization in decision trees (\texttt{min\_samples\_leaf=10}).

\begin{table}[!ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c}
\hline
$N$ & $\text{MSE}_\text{train, Y}$ & $\text{MSE}_\text{test, Y}$ & $\text{MSE}_\text{train, N}$ & $\text{MSE}_\text{test, N}$ \\
\hline
50    & \cellcolor{mygreen} 0.00 & \cellcolor{myred} 2.61 & \cellcolor{mygreen} 2.54 & \cellcolor{myred} 3.07 \\
100   & \cellcolor{mygreen} 0.00 & \cellcolor{myred} 1.69 & \cellcolor{mygreen} 0.87 & \cellcolor{myred} 1.34 \\
200   & \cellcolor{mygreen} 0.00 & \cellcolor{myred} 2.35 & \cellcolor{mygreen} 0.73 & \cellcolor{myred} 1.56 \\
500   & \cellcolor{mygreen} 0.00 & \cellcolor{myred} 1.92 & \cellcolor{mygreen} 0.89 & \cellcolor{myred} 1.03 \\
1000  & \cellcolor{mygreen} 0.00 & \cellcolor{myred} 1.70 & \cellcolor{mygreen} 0.82 & \cellcolor{myred} 1.23 \\
2000  & \cellcolor{mygreen} 0.00 & \cellcolor{myred} 2.17 & \cellcolor{mygreen} 0.79 & \cellcolor{myred} 1.33 \\
5000  & \cellcolor{mygreen} 0.00 & \cellcolor{myred} 1.89 & \cellcolor{mygreen} 0.79 & \cellcolor{myred} 1.13 \\
\hline
\end{tabular}%
}
\caption{Comparison of Tree Models (Y = with regularization, N = without regularization) for Different Sample Sizes $N$.}
\end{table}

\paragraph{TODO 4} Here we will try to see which other hyperparameters we can change.

\begin{lstlisting}
rf_no_hyp, rf_with_hyp = [], []

for X_train, X_test, y_train, y_test in results_linear:
    rf_no_hyp_i = RandomForestRegressor(n_estimators=100, random_state=42)
    
    # using hyperparameters
    rf_with_hyp_i = RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        min_samples_split=10,
        min_samples_leaf=5,
        max_features=0.5,
        random_state=42
    )
    
    rf_no_hyp_i.fit(X_train, y_train)
    rf_with_hyp_i.fit(X_train, y_train)
    
    rf_no_hyp.append(rf_no_hyp_i)
    rf_with_hyp.append(rf_with_hyp_i)


for rf_no_hyp_i, rf_with_hyp_i, result in zip(rf_no_hyp, rf_with_hyp, results_linear):
    X_train, X_test, y_train, y_test = result
    plt.figure(figsize=(10, 4))
    
    plt.subplot(1, 2, 1)
    plot_regression_predictions(rf_no_hyp_i, X_train, y_train)
    plt.title("Random Forest (default)")
    
    plt.subplot(1, 2, 2)
    plot_regression_predictions(rf_with_hyp_i, X_train, y_train)
    plt.title("Random Forest (strong regularization)")
    
    plt.show()

results_rf_no_hyp = []
results_rf_with_hyp = []

for result, rf_no_hyp, rf_with_hyp in zip(results_linear, rf_no_hyp, rf_with_hyp):
    X_train, X_test, y_train, y_test = result
    
    # Model without hyperparameters
    y_train_pred = rf_no_hyp.predict(X_train)
    y_test_pred = rf_no_hyp.predict(X_test)
    mse_train_no_hyp = mean_squared_error(y_train, y_train_pred)
    mse_test_no_hyp = mean_squared_error(y_test, y_test_pred)
    results_rf_no_hyp.append([mse_train_no_hyp, mse_test_no_hyp])
    
    # Model with hyperparameters
    y_train_pred = rf_with_hyp.predict(X_train)
    y_test_pred = rf_with_hyp.predict(X_test)
    mse_train_with_hyp = mean_squared_error(y_train, y_train_pred)
    mse_test_with_hyp = mean_squared_error(y_test, y_test_pred)
    results_rf_with_hyp.append([mse_train_with_hyp, mse_test_with_hyp])

# Print results
for sample, ris_no_hyp, ris_with_hyp in zip(samples, results_rf_no_hyp, results_rf_with_hyp):
    mse_train_no_hyp, mse_test_no_hyp = ris_no_hyp
    mse_train_with_hyp, mse_test_with_hyp = ris_with_hyp
    print(f"RESULTS FOR {sample} instances:")
    print(f"   Default RF:      Train MSE = {mse_train_no_hyp:.4f},  Test MSE = {mse_test_no_hyp:.4f}")
    print(f"   Regularized RF:  Train MSE = {mse_train_with_hyp:.4f},  Test MSE = {mse_test_with_hyp:.4f}")
    print(f"   Overfitting reduced? {'YES' if mse_test_with_hyp < mse_test_no_hyp else 'NO'}\n")
\end{lstlisting}


\begin{table}[!ht]
\centering
\scriptsize
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\hline
$N$ & \text{Default RF Train MSE} & \text{Default RF Test MSE} & \text{Regularized RF Train MSE} & \text{Regularized RF Test MSE} & \text{Overfitting Reduced?} \\
\hline
50    & 0.2963 & 1.3113 & 1.3545 & 1.6156 & NO \\
100   & 0.1761 & 1.5028 & 0.7173 & 1.2067 & YES \\
200   & 0.1806 & 1.9127 & 0.6637 & 1.5084 & YES \\
500   & 0.1972 & 1.4768 & 0.8058 & 1.0025 & YES \\
1000  & 0.2117 & 1.4009 & 0.7574 & 1.1977 & YES \\
2000  & 0.1912 & 1.6286 & 0.7572 & 1.1690 & YES \\
5000  & 0.1979 & 1.4017 & 0.7985 & 1.0554 & YES \\
\hline
\end{tabular}%
}
\caption{Comparison of Default and Regularized Random Forest Models for Different Sample Sizes $N$.}
\end{table}


\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/rf_comparison_50.jpg}
    \caption{50 samples}
    \label{fig:rf_comparison_50}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/rf_comparison_100.jpg}
    \caption{100 samples}
    \label{fig:rf_comparison_100}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/rf_comparison_200.jpg}
    \caption{200 samples}
    \label{fig:rf_comparison_200}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/rf_comparison_500.jpg}
    \caption{500 samples}
    \label{fig:rf_comparison_500}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/rf_comparison_1000.jpg}
    \caption{1000 samples}
    \label{fig:rf_comparison_1000}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/rf_comparison_2000.jpg}
    \caption{2000 samples}
    \label{fig:rf_comparison_2000}
\end{figure}

\paragraph{TODO 5} Now let's see the effect of changing just the value \texttt{n\_estimators}. Observations:
\begin{itemize}
    \item Training MSE decreases (or stabilizes) as n\_estimators grows
    \item Test MSE usually decreases at first, then plateaus
    \item Optimal n\_estimators = point where test MSE stops improving significantly
\end{itemize}

\begin{lstlisting}
n_estimators_values = [20, 50, 100, 200, 300]
train_mses = []
test_mses = []

X_train, X_test, y_train, y_test = results_linear[-1]  # take the biggest split (results_linear has all datasets with increasing samples size)

for n in n_estimators_values:
    rf = RandomForestRegressor(n_estimators=n, random_state=42)
    rf.fit(X_train, y_train)
    
    train_pred = rf.predict(X_train)
    test_pred = rf.predict(X_test)
    
    train_mses.append(mean_squared_error(y_train, train_pred))
    test_mses.append(mean_squared_error(y_test, test_pred))

# Plot
plt.figure(figsize=(8, 5))
plt.plot(n_estimators_values, train_mses, 'o-', label='Training MSE', color='blue')
plt.plot(n_estimators_values, test_mses, 'o-', label='Test MSE', color='red')
plt.xlabel('n_estimators')
plt.ylabel('Mean Squared Error')
plt.title('Effect of n_estimators on Random Forest Performance')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/n_estimators_comparison.jpg}
    \label{fig:n_estimators_comparison}
\end{figure}

\paragraph{TODO 6 (optional)} Let's do the same thing once again, but changing another hyperparameter. In this case we did with: \texttt{max\_depth\_value}.

\begin{lstlisting}
max_depth_values = [3, 5, 8, 10, 15, 20, None]
train_mses_depth = []
test_mses_depth = []

for md in max_depth_values:
    rf = RandomForestRegressor(n_estimators=200, max_depth=md, random_state=42)
    rf.fit(X_train, y_train)
    
    train_mses_depth.append(mean_squared_error(y_train, rf.predict(X_train)))
    test_mses_depth.append(mean_squared_error(y_test, rf.predict(X_test)))

plt.figure(figsize=(8, 5))
plt.plot([str(d) for d in max_depth_values], train_mses_depth, 'o-', label='Training MSE')
plt.plot([str(d) for d in max_depth_values], test_mses_depth, 'o-', label='Test MSE')
plt.xlabel('max_depth')
plt.ylabel('MSE')
plt.title('Random Forest Performance vs max_depth')
plt.legend()
plt.grid(True, alpha=0.3)
plt.xticks(rotation=45)
plt.show()
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{images/max_depth_comparison.jpg}
    \label{fig:max_depth_comparison}
\end{figure}

\subsection{Module 2}

\paragraph{TODO 1} We test our own value to check the train dataset.

\begin{lstlisting}
    y_new_prediction = log_reg.predict([[6.0]]) # expected 1 -> Virginica
    print(f"The flower is {"" if y_new_prediction else "NOT "}Virginica")
\end{lstlisting}
output: \texttt{The flower is Virginica}

\paragraph{TODO 2} We also want to check the related probability, or in other words, how secure is the answer of the model:

\begin{lstlisting}
    y_proba_1 = log_reg.predict_proba([[6.0]])
    y_proba_1
\end{lstlisting}
output: \texttt{array([[6.83166501e-09, 9.99999993e-01]])}, meaning the model is 99.9999993\% sure the answer is the flower is Virginica.

\paragraph{TODO 3} Obtain the confusion matrices of the model:

\begin{lstlisting}
# we choose the training on the petal length
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix



X = iris["data"][:, 2][:, None]  # petal length, take the column n 2
y = (iris["target"] == 2).astype(np.int64)  # this tests if X is = 2. 1 = True if Iris virginica, else 0 = False
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y) # we divide in test and train sets

log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train, y_train)

y_train_pred = log_reg.predict(X_train)
y_test_pred = log_reg.predict(X_test)

j = confusion_matrix(y_train, y_train_pred)
k = confusion_matrix(y_test, y_test_pred)
print(j, "\n",k)
\end{lstlisting}

\noindent output:

\[
j = \begin{pmatrix}
77 & 3 \\
2 & 38
\end{pmatrix}
\qquad
k = \begin{pmatrix}
19 & 1 \\
1 & 9
\end{pmatrix}
\]

\noindent comments:
\begin{itemize}
    \item The values on the anti-diagonal are low and similar between each other. This means a good accuracy  because we have a low number of false positives and false negatives, and are not really different (3, 2 and 1,1) which means that probably the model is not biased.
    \item After analyzing the dataset distribution, however, we see that Virginica is only 1/3 of the whole dataset; this means that the values of false negatives and false positives cannot be taken like that when the dataset is unbalanced
    \item As we expected, we obtained that the true positives are about 33\% of the set, while the remaining are the true negatives. 
    \item This is because 1/3 of the dataset is Virginica (true positive) and the remaining 2/3 are not Virginica (true negatives)
\end{itemize}

\paragraph{TODO 4} Repeat the previous experience but with all the attributes all-together.

\begin{lstlisting}
X = iris["data"] # we consider all the features
y = (iris["target"] == 2).astype(np.int64)  # this tests if X is = 2. 1 = True if Iris virginica, else 0 = False
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y) # we divide in test and train sets

log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train, y_train)

y_train_pred = log_reg.predict(X_train)
y_test_pred = log_reg.predict(X_test)

l = confusion_matrix(y_train, y_train_pred)
m = confusion_matrix(y_test, y_test_pred)
print(l, "\n",m)
\end{lstlisting}

\noindent output:
\[
l = \begin{pmatrix}
78 & 2 \\
1 & 39
\end{pmatrix}
\qquad
m = \begin{pmatrix}
20 & 0 \\
0 & 10
\end{pmatrix}
\]

\noindent comments:
\begin{itemize}
    \item We see a slight improvement of the false positives and false negatives, especially in the test set: we have zeros on the anti-diagonal.
\end{itemize}

\subsection{Module 3}

\paragraph{TODO 1} We test our own value to check the train dataset.

\begin{lstlisting}
y_new_1 = softmax_reg.predict([[4.0, 1]])    # petal length 1.0, petal width 0.5
y_new_1
\end{lstlisting}
output: \texttt{array([1])}

\paragraph{TODO 2} We also want to check the related probability, or in other words, how secure is the answer of the model:

\begin{lstlisting}
y_proba_1 = softmax_reg.predict_proba([[4.0, 1]])
y_proba_1
\end{lstlisting}
output: \texttt{array([[0.02404399, 0.96249222, 0.01346379]])}, meaning the model is 96.249222\% sure the answer is the flower is Virginica. Other probabilities are reffered to `setosa' and `versicolor'.

\paragraph{\textcolor{red}{TODO 3}}
\paragraph{\textcolor{red}{TODO 4}}
\paragraph{\textcolor{red}{TODO 5}}